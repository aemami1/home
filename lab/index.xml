<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Labs | Ali Emami</title><link>https://aemami.ca/lab/</link><atom:link href="https://aemami.ca/lab/index.xml" rel="self" type="application/rss+xml"/><description>Labs</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2024 Ali Emami</copyright><image><url>https://aemami.ca/media/icon_hu29bb0f8250f761abddc65535b755b381_104744_512x512_fill_lanczos_center_3.png</url><title>Labs</title><link>https://aemami.ca/lab/</link></image><item><title/><link>https://aemami.ca/lab/lablist/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aemami.ca/lab/lablist/</guid><description>&lt;h2 id="welcome-to-brock-nlp-lab">Welcome to Brock NLP Lab!&lt;/h2>
&lt;h1 id="lab-members">Lab Members&lt;/h1>
&lt;h4 id="director">Director&lt;/h4>
&lt;p>Ali Emami, Assistant Professor of Computer Science&lt;/p>
&lt;h4 id="msc-students">MSc Students&lt;/h4>
&lt;ul>
&lt;li>Robert Morabito (2022-2023, Undergraduate; 2024-Present, MSc)&lt;/li>
&lt;li>Kaige Chen (Fall 2024 – Present)&lt;/li>
&lt;li>Kazi Nishat Anwar (Fall 2024 – Present)&lt;/li>
&lt;li>Nikta Gohari Sadr (Fall 2023 – Present)&lt;/li>
&lt;li>Sarfaroz Yunusov (Fall 2023 – Present)&lt;/li>
&lt;li>Abhishek Kumar (Fall 2023 – Summer 2024)&lt;/li>
&lt;/ul>
&lt;h4 id="undergraduate-researchers">Undergraduate Researchers&lt;/h4>
&lt;ul>
&lt;li>Tyler Mcdonald (Summer 2023 – Present, &lt;em>NSERC Undergraduate Student Research Awardee&lt;/em>)&lt;/li>
&lt;li>Sangmitra Madhusudan (Summer 2024 - Present, &lt;em>Brock Co-op Program&lt;/em>)&lt;/li>
&lt;li>Skye Reid (Summer 2024)&lt;/li>
&lt;li>QiQi Gao (Summer 2022 – Summer 2023)&lt;/li>
&lt;/ul>
&lt;h4 id="summer-researchers-mitacs-globalink-interns">Summer Researchers (Mitacs Globalink Interns)&lt;/h4>
&lt;ul>
&lt;li>Ghofrane Faidi (Summer 2024)&lt;/li>
&lt;li>Angel Loredo (Summer 2024)&lt;/li>
&lt;li>Harsh Lalai (Summer 2024)&lt;/li>
&lt;/ul>
&lt;figure id="figure-june-2024-niagara-falls-canada">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="../uploads/group.jpg" alt="June, 2024, Niagara Falls, Canada" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
June, 2024, Niagara Falls, Canada
&lt;/figcaption>&lt;/figure>
&lt;h1 id="research">Research&lt;/h1>
&lt;p>The Brock NLP lab is dedicated to developing &lt;strong>trustworthy, generalizable, transparent, and fair&lt;/strong> natural language processing tools capable of understanding, reasoning, and producing human-like text. Our research spans multiple facets of AI, with a particular focus on three key areas:&lt;/p>
&lt;h3 id="1-bias-detection-and-mitigation-in-large-language-models-llms">1. Bias Detection and Mitigation in Large Language Models (LLMs)&lt;/h3>
&lt;p>We develop innovative methods to identify and address subtle biases in LLMs, aiming to create more equitable AI systems. Our work introduces novel metrics and evaluation frameworks to measure representative and affinity biases that often go unnoticed.&lt;/p>
&lt;figure id="figure-proportion-of-gpt-4s-preferred-responses-for-the-short-poem-task-in-cogs-categorized-by-identity-specific-prompts-with-highlighted-sectors-indicating-a-preference-for-outputs-from-those-identities-read-more-about-this-studypublicationkumar-2024-subtle">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="../uploads/subtle.png" alt="Proportion of GPT-4’s preferred responses for the short poem task in CoGS, categorized by identity-specific prompts, with highlighted sectors indicating a preference for outputs from those identities. [Read more about this study](/publication/kumar-2024-subtle/)." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Proportion of GPT-4’s preferred responses for the short poem task in CoGS, categorized by identity-specific prompts, with highlighted sectors indicating a preference for outputs from those identities. &lt;a href="../publication/kumar-2024-subtle/">Read more about this study&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-framework-checklist-comparing-the-consistency-of-recent-debiasing-methods--read-more-about-this-studypublicationmorabito-2023-debiasing">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="../uploads/debiasing.png" alt="Framework checklist comparing the consistency of recent debiasing methods. [Read more about this study](/publication/morabito-2023-debiasing/)." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Framework checklist comparing the consistency of recent debiasing methods. &lt;a href="../publication/morabito-2023-debiasing/">Read more about this study&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;p>&lt;strong>Key Contributions:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Introduced the Representative Bias Score (RBS) and Affinity Bias Score (ABS) to measure subtle biases in LLMs.&lt;/li>
&lt;li>Developed the Creativity-Oriented Generation Suite (CoGS) for detecting biases in open-ended tasks.&lt;/li>
&lt;li>Proposed a protocol for measuring the consistency of debiasing techniques in language models.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Recent Publications:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Kumar, A., Yunusov, S., Emami, A. (2024). Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models. In &lt;em>Proceedings of ACL 2024&lt;/em>.&lt;/li>
&lt;li>Morabito, R., Kabbara, J., Emami, A. (2023). Debiasing should be Good and Bad: Measuring the Consistency of Debiasing Techniques in Language Models. In &lt;em>Findings of ACL 2023&lt;/em>.&lt;/li>
&lt;/ul>
&lt;h3 id="2-reasoning-and-benchmarking-of-llms">2. Reasoning and Benchmarking of LLMs&lt;/h3>
&lt;p>We create innovative challenges and datasets to rigorously test the reasoning capabilities of LLMs, with a particular focus on enhancing and expanding the Winograd Schema Challenge (WSC).&lt;/p>
&lt;figure id="figure-a-representative-output-from-stable-diffusion-20-on-a-winovis-instance-the-diffusion-attentive-attribution-maps-daam-clarify-the-models-focus-for-different-terms-and-the-correctness-of-its-interpretation-correctly-identifying-bee-and-flower-but-erroneously-associating-it-with-the-bee-instead-of-the-flower-read-more-about-this-studypublicationpark-2024-winovis">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="../uploads/winovis.jpg" alt="A representative output from Stable Diffusion 2.0 on a WINOVIS instance. The Diffusion Attentive Attribution Maps (DAAM) clarify the model’s focus for different terms and the correctness of its interpretation: correctly identifying ‘bee’ and ‘flower’ but erroneously associating ‘it’ with the bee instead of the flower. [Read more about this study](/publication/park-2024-winovis/)." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
A representative output from Stable Diffusion 2.0 on a WINOVIS instance. The Diffusion Attentive Attribution Maps (DAAM) clarify the model’s focus for different terms and the correctness of its interpretation: correctly identifying ‘bee’ and ‘flower’ but erroneously associating ‘it’ with the bee instead of the flower. &lt;a href="../publication/park-2024-winovis/">Read more about this study&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-interface-of-evograd-at-httpswwwevogradcomhttpsevogradcom-read-more-about-this-studypublicationsun-2024-evo">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="../uploads/evograd.png" alt="Interface of EvoGrad at [https://www.evograd.com/](https://evograd.com). [Read more about this study](/publication/sun-2024-evo/)." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Interface of EvoGrad at &lt;a href="https://evograd.com" target="_blank" rel="noopener">https://www.evograd.com/&lt;/a>. &lt;a href="../publication/sun-2024-evo/">Read more about this study&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-overview-of-the-wsc-generation-and-evaluation-processes-on-the-left-the-flowchart-depicts-the-wsc-generation-process-using-a-real-example-generated-by-gpt-4-on-the-right-a-wsc-instance-evaluation-contrasts-the-outcomes-of-standard-prompting-and-our-tree-of-experts-prompting--read-more-about-this-studypublicationzahraei-2024-wsc">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="../uploads/wsc&amp;#43;.png" alt="Overview of the WSC&amp;#43; generation and evaluation processes. On the left, the flowchart depicts the WSC&amp;#43; generation process, using a real example generated by GPT-4. On the right, a WSC&amp;#43; instance evaluation contrasts the outcomes of standard prompting and our Tree-of-Experts prompting. [Read more about this study](/publication/zahraei-2024-wsc/)." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Overview of the WSC+ generation and evaluation processes. On the left, the flowchart depicts the WSC+ generation process, using a real example generated by GPT-4. On the right, a WSC+ instance evaluation contrasts the outcomes of standard prompting and our Tree-of-Experts prompting. &lt;a href="../publication/zahraei-2024-wsc/">Read more about this study&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;p>&lt;strong>Key Contributions:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Developed WinoVis, a novel dataset for probing text-to-image models on pronoun disambiguation in multimodal contexts.&lt;/li>
&lt;li>Created EvoGrad, an open-source platform for dynamic WSC datasets using a human-in-the-loop approach.&lt;/li>
&lt;li>Introduced WSC+, an enhanced version of the WSC using a Tree-of-Experts approach.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Recent Publications:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Park, B., Janecek, M., Li, Y., Emami, A. (2024). Picturing Ambiguity: A Visual Twist on the Winograd Schema Challenge. In &lt;em>Proceedings of ACL 2024&lt;/em>.&lt;/li>
&lt;li>Sun, J.H., &amp;amp; Emami, A. (2024). EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human Adversaries. In &lt;em>Proceedings of COLING-LREC 2024&lt;/em>.&lt;/li>
&lt;li>Zahraei, P.S., &amp;amp; Emami, A. (2024). WSC+: Enhancing The Winograd Schema Challenge Using Tree-of-Experts. In &lt;em>Proceedings of EACL 2024&lt;/em>.&lt;/li>
&lt;/ul>
&lt;h3 id="3-machine-interpretability-and-confidence-analysis">3. Machine Interpretability and Confidence Analysis&lt;/h3>
&lt;p>We investigate the inner workings of LLMs, focusing on their confidence-probability alignment and decision-making processes to enhance their reliability and interpretability.&lt;/p>
&lt;figure id="figure-flow-diagram-illustrating-the-process-of-extracting-and-comparing-the-internal-confidence-and-verbalized-certainty-in-an-llm-read-more-about-this-studypublicationkumar-2024-confidence">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="../uploads/confidence.png" alt="Flow diagram illustrating the process of extracting and comparing the Internal Confidence and Verbalized Certainty in an LLM. [Read more about this study](/publication/kumar-2024-confidence/)." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Flow diagram illustrating the process of extracting and comparing the Internal Confidence and Verbalized Certainty in an LLM. &lt;a href="../publication/kumar-2024-confidence/">Read more about this study&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;p>&lt;strong>Key Contributions:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Introduced the concept of Confidence-Probability Alignment in LLMs.&lt;/li>
&lt;li>Developed novel prompting techniques to encourage model introspection and self-evaluation.&lt;/li>
&lt;li>Proposed a framework for assessing model stability in dynamic tasks through the error depth metric.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Recent Publication:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Kumar, A., Morabito, R., Umbet, S., Kabbara, J., Emami, A. (2024). Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models. In &lt;em>Proceedings of ACL 2024&lt;/em>.&lt;/li>
&lt;/ul>
&lt;p>We are dedicated to advancing the development of more reliable, unbiased, and interpretable language models, with our work regularly presented at conferences such as ACL, COLING-LREC, and EACL.&lt;/p>
&lt;h1 id="map-of-student-origins">Map of Student Origins&lt;/h1>
&lt;link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css"/>
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css"/>
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet.markercluster/1.1.0/MarkerCluster.css"/>
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet.markercluster/1.1.0/MarkerCluster.Default.css"/>
&lt;script src="https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js">&lt;/script>
&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js">&lt;/script>
&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet.markercluster/1.1.0/leaflet.markercluster.js">&lt;/script>
&lt;div class="folium-map" id="map_9f3fcd549824baa231d01b8c91fbf082" style="height: 400px;">&lt;/div>
&lt;script>
var map = L.map('map_9f3fcd549824baa231d01b8c91fbf082', {
center: [20, 0],
zoom: 2
});
L.tileLayer('https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', {
attribution: 'Map data &amp;copy; &lt;a href="https://www.openstreetmap.org/copyright">OpenStreetMap&lt;/a> contributors'
}).addTo(map);
var locations = [
{lat: 35.70746143717422, lng: 51.34695391303935, popup: "Nikta Gohari Sadr"},
{lat: 19.072572898510337, lng: 72.87790269902361, popup: "Harsh Lalai"},
{lat: 43.36129389343648, lng: -80.30641201651625, popup: "Tyler McDonald"},
{lat: 42.9922, lng: -79.2483, popup: "Robert Morabito"},
{lat: 23.81324948086691, lng: 90.41422236172856, popup: "Kazi Nishat Anwar"},
{lat: 26.283020330537518, lng: 80.4161059621779, popup: "Abhishek Kumar"},
{lat: 19.436753354471875, lng: -99.14361078026432, popup: "Angel Loredo"},
{lat: 28.61764032564273, lng: 77.21234851645079, popup: "Sangmitra Madhusudan"},
{lat: 26.283020330537518, lng: 80.4161059621779, popup: "Abhishek Kumar"},
{lat: 24.94995291735982, lng: 118.6783729252374, popup: "Kaige Chen"},
{lat: 43.15951170805985, lng: -79.24680006638461, popup: "Skye Reid"},
{lat: 37.509975093912196, lng: 71.59398400486116, popup: "Sarfaroz Yunusov"},
];
locations.forEach(function(location) {
L.marker([location.lat, location.lng]).addTo(map)
.bindPopup(location.popup);
});
&lt;/script>
&lt;h1 id="join-us">Join Us&lt;/h1>
&lt;p>We are recruiting new graduate students for Fall, 2024&lt;/p>
&lt;p>&lt;strong>Undergraduates:&lt;/strong> Please don&amp;rsquo;t hesitate to email me to inquire about research projects that I (or better, yet, you) may have in mind. Please also attach your transcript as well as a brief description of which areas of my research interests (e.g., natural language processing) you would like to work on and why. I highly encourage, and prefer, students that are planning on a summer internship (under the NSERC USRA or SURA program), or are planning to do an Honour&amp;rsquo;s thesis.&lt;/p>
&lt;p>&lt;strong>Graduates:&lt;/strong> M.Sc. (&lt;!-- raw HTML omitted -->Computer Science&lt;!-- raw HTML omitted -->) and PhD (&lt;!-- raw HTML omitted -->Intelligent Systems and Data Science&lt;!-- raw HTML omitted -->) admissions are handled centrally in our department. Please see &lt;a href="https://brocku.ca/graduate-studies/future-students/apply/" target="_blank" rel="noopener">this&lt;/a> page for application instructions.&lt;/p></description></item><item><title>Lab</title><link>https://aemami.ca/lab/lab/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://aemami.ca/lab/lab/</guid><description>
&lt;hr>
&lt;h2 id="welcome-to-brock-nlp-lab">Welcome to Brock NLP Lab!&lt;/h2>
&lt;h1 id="lab-members">Lab Members&lt;/h1>
&lt;h4 id="director">Director&lt;/h4>
&lt;p>Ali Emami, Assistant Professor of Computer Science&lt;/p>
&lt;h4 id="msc-students">MSc Students&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>Robert Morabito (2022-2023, Undergraduate; 2024-Present, MSc)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Kaige Chen (Fall 2024 – Present)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Kazi Nishat Anwar (Fall 2024 – Present)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Nikta Gohari Sadr (Fall 2023 – Present)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Sarfaroz Yunusov (Fall 2023 – Present)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Abhishek Kumar (Fall 2023 – Summer 2024)&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="undergraduate-researchers">Undergraduate Researchers&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>Tyler Mcdonald (Summer 2023 – Present, &lt;em>NSERC Undergraduate Student Research Awardee&lt;/em>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Sangmitra Madhusudan (Summer 2024 - Present, &lt;em>Brock Co-op Program&lt;/em>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Skye Reid (Summer 2024)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>QiQi Gao (Summer 2022 – Summer 2023)&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="summer-researchers-mitacs-globalink-interns">Summer Researchers (Mitacs Globalink Interns)&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>Ghofrane Faidi (Summer 2024)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Angel Loredo (Summer 2024)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Harsh Lalai (Summer 2024)&lt;/p>
&lt;/li>
&lt;/ul>
&lt;figure id="figure-june-2024-niagara-falls-canada">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="../uploads/group.jpg" alt="June, 2024, Niagara Falls, Canada" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
June, 2024, Niagara Falls, Canada
&lt;/figcaption>&lt;/figure>
&lt;h1 id="research">Research&lt;/h1>
&lt;p>The Brock NLP lab is dedicated to developing &lt;strong>trustworthy, generalizable, transparent, and fair&lt;/strong> natural language processing tools capable of understanding, reasoning, and producing human-like text. Our research spans multiple facets of AI, with a particular focus on three key areas:&lt;/p>
&lt;h3 id="1-bias-detection-and-mitigation-in-large-language-models-llms">1. Bias Detection and Mitigation in Large Language Models (LLMs)&lt;/h3>
&lt;p>We develop innovative methods to identify and address subtle biases in LLMs, aiming to create more equitable AI systems. Our work introduces novel metrics and evaluation frameworks to measure representative and affinity biases that often go unnoticed.&lt;/p>
&lt;figure id="figure-proportion-of-gpt-4s-preferred-responses-for-the-short-poem-task-in-cogs-categorized-by-identity-specific-prompts-with-highlighted-sectors-indicating-a-preference-for-outputs-from-those-identities-read-more-about-this-studypublicationkumar-2024-subtle">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="../uploads/subtle.png" alt="Proportion of GPT-4’s preferred responses for the short poem task in CoGS, categorized by identity-specific prompts, with highlighted sectors indicating a preference for outputs from those identities. [Read more about this study](/publication/kumar-2024-subtle/)." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Proportion of GPT-4’s preferred responses for the short poem task in CoGS, categorized by identity-specific prompts, with highlighted sectors indicating a preference for outputs from those identities. &lt;a href="../publication/kumar-2024-subtle/">Read more about this study&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-framework-checklist-comparing-the-consistency-of-recent-debiasing-methods--read-more-about-this-studypublicationmorabito-2023-debiasing">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="../uploads/debiasing.png" alt="Framework checklist comparing the consistency of recent debiasing methods. [Read more about this study](/publication/morabito-2023-debiasing/)." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Framework checklist comparing the consistency of recent debiasing methods. &lt;a href="../publication/morabito-2023-debiasing/">Read more about this study&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;p>&lt;strong>Key Contributions:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Introduced the Representative Bias Score (RBS) and Affinity Bias Score (ABS) to measure subtle biases in LLMs.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Developed the Creativity-Oriented Generation Suite (CoGS) for detecting biases in open-ended tasks.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Proposed a protocol for measuring the consistency of debiasing techniques in language models.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Recent Publications:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Kumar, A., Yunusov, S., Emami, A. (2024). Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models. In &lt;em>Proceedings of ACL 2024&lt;/em>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Morabito, R., Kabbara, J., Emami, A. (2023). Debiasing should be Good and Bad: Measuring the Consistency of Debiasing Techniques in Language Models. In &lt;em>Findings of ACL 2023&lt;/em>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="2-reasoning-and-benchmarking-of-llms">2. Reasoning and Benchmarking of LLMs&lt;/h3>
&lt;p>We create innovative challenges and datasets to rigorously test the reasoning capabilities of LLMs, with a particular focus on enhancing and expanding the Winograd Schema Challenge (WSC).&lt;/p>
&lt;figure id="figure-a-representative-output-from-stable-diffusion-20-on-a-winovis-instance-the-diffusion-attentive-attribution-maps-daam-clarify-the-models-focus-for-different-terms-and-the-correctness-of-its-interpretation-correctly-identifying-bee-and-flower-but-erroneously-associating-it-with-the-bee-instead-of-the-flower-read-more-about-this-studypublicationpark-2024-winovis">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="../uploads/winovis.jpg" alt="A representative output from Stable Diffusion 2.0 on a WINOVIS instance. The Diffusion Attentive Attribution Maps (DAAM) clarify the model’s focus for different terms and the correctness of its interpretation: correctly identifying ‘bee’ and ‘flower’ but erroneously associating ‘it’ with the bee instead of the flower. [Read more about this study](/publication/park-2024-winovis/)." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
A representative output from Stable Diffusion 2.0 on a WINOVIS instance. The Diffusion Attentive Attribution Maps (DAAM) clarify the model’s focus for different terms and the correctness of its interpretation: correctly identifying ‘bee’ and ‘flower’ but erroneously associating ‘it’ with the bee instead of the flower. &lt;a href="../publication/park-2024-winovis/">Read more about this study&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-interface-of-evograd-at-httpswwwevogradcomhttpsevogradcom-read-more-about-this-studypublicationsun-2024-evo">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="../uploads/evograd.png" alt="Interface of EvoGrad at [https://www.evograd.com/](https://evograd.com). [Read more about this study](/publication/sun-2024-evo/)." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Interface of EvoGrad at &lt;a href="https://evograd.com" target="_blank" rel="noopener">https://www.evograd.com/&lt;/a>. &lt;a href="../publication/sun-2024-evo/">Read more about this study&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-overview-of-the-wsc-generation-and-evaluation-processes-on-the-left-the-flowchart-depicts-the-wsc-generation-process-using-a-real-example-generated-by-gpt-4-on-the-right-a-wsc-instance-evaluation-contrasts-the-outcomes-of-standard-prompting-and-our-tree-of-experts-prompting--read-more-about-this-studypublicationzahraei-2024-wsc">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="../uploads/wsc&amp;#43;.png" alt="Overview of the WSC&amp;#43; generation and evaluation processes. On the left, the flowchart depicts the WSC&amp;#43; generation process, using a real example generated by GPT-4. On the right, a WSC&amp;#43; instance evaluation contrasts the outcomes of standard prompting and our Tree-of-Experts prompting. [Read more about this study](/publication/zahraei-2024-wsc/)." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Overview of the WSC+ generation and evaluation processes. On the left, the flowchart depicts the WSC+ generation process, using a real example generated by GPT-4. On the right, a WSC+ instance evaluation contrasts the outcomes of standard prompting and our Tree-of-Experts prompting. &lt;a href="../publication/zahraei-2024-wsc/">Read more about this study&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;p>&lt;strong>Key Contributions:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Developed WinoVis, a novel dataset for probing text-to-image models on pronoun disambiguation in multimodal contexts.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Created EvoGrad, an open-source platform for dynamic WSC datasets using a human-in-the-loop approach.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Introduced WSC+, an enhanced version of the WSC using a Tree-of-Experts approach.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Recent Publications:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Park, B., Janecek, M., Li, Y., Emami, A. (2024). Picturing Ambiguity: A Visual Twist on the Winograd Schema Challenge. In &lt;em>Proceedings of ACL 2024&lt;/em>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Sun, J.H., &amp;amp; Emami, A. (2024). EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human Adversaries. In &lt;em>Proceedings of COLING-LREC 2024&lt;/em>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Zahraei, P.S., &amp;amp; Emami, A. (2024). WSC+: Enhancing The Winograd Schema Challenge Using Tree-of-Experts. In &lt;em>Proceedings of EACL 2024&lt;/em>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="3-machine-interpretability-and-confidence-analysis">3. Machine Interpretability and Confidence Analysis&lt;/h3>
&lt;p>We investigate the inner workings of LLMs, focusing on their confidence-probability alignment and decision-making processes to enhance their reliability and interpretability.&lt;/p>
&lt;figure id="figure-flow-diagram-illustrating-the-process-of-extracting-and-comparing-the-internal-confidence-and-verbalized-certainty-in-an-llm-read-more-about-this-studypublicationkumar-2024-confidence">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="../uploads/confidence.png" alt="Flow diagram illustrating the process of extracting and comparing the Internal Confidence and Verbalized Certainty in an LLM. [Read more about this study](/publication/kumar-2024-confidence/)." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Flow diagram illustrating the process of extracting and comparing the Internal Confidence and Verbalized Certainty in an LLM. &lt;a href="../publication/kumar-2024-confidence/">Read more about this study&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;p>&lt;strong>Key Contributions:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Introduced the concept of Confidence-Probability Alignment in LLMs.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Developed novel prompting techniques to encourage model introspection and self-evaluation.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Proposed a framework for assessing model stability in dynamic tasks through the error depth metric.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Recent Publication:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Kumar, A., Morabito, R., Umbet, S., Kabbara, J., Emami, A. (2024). Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models. In &lt;em>Proceedings of ACL 2024&lt;/em>.&lt;/li>
&lt;/ul>
&lt;p>We are dedicated to advancing the development of more reliable, unbiased, and interpretable language models, with our work regularly presented at conferences such as ACL, COLING-LREC, and EACL.&lt;/p>
&lt;h1 id="map-of-student-origins">Map of Student Origins&lt;/h1>
&lt;link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css"/>
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css"/>
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet.markercluster/1.1.0/MarkerCluster.css"/>
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet.markercluster/1.1.0/MarkerCluster.Default.css"/>
&lt;script src="https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js">&lt;/script>
&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js">&lt;/script>
&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet.markercluster/1.1.0/leaflet.markercluster.js">&lt;/script>
&lt;div class="folium-map" id="map_9f3fcd549824baa231d01b8c91fbf082" style="height: 400px;">&lt;/div>
&lt;script>
var map = L.map('map_9f3fcd549824baa231d01b8c91fbf082', {
center: [20, 0],
zoom: 2
});
L.tileLayer('https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', {
attribution: 'Map data &amp;copy; &lt;a href="https://www.openstreetmap.org/copyright">OpenStreetMap&lt;/a> contributors'
}).addTo(map);
var locations = [
{lat: 35.70746143717422, lng: 51.34695391303935, popup: "Nikta Gohari Sadr"},
{lat: 19.072572898510337, lng: 72.87790269902361, popup: "Harsh Lalai"},
{lat: 43.36129389343648, lng: -80.30641201651625, popup: "Tyler McDonald"},
{lat: 42.9922, lng: -79.2483, popup: "Robert Morabito"},
{lat: 23.81324948086691, lng: 90.41422236172856, popup: "Kazi Nishat Anwar"},
{lat: 26.283020330537518, lng: 80.4161059621779, popup: "Abhishek Kumar"},
{lat: 19.436753354471875, lng: -99.14361078026432, popup: "Angel Loredo"},
{lat: 28.61764032564273, lng: 77.21234851645079, popup: "Sangmitra Madhusudan"},
{lat: 26.283020330537518, lng: 80.4161059621779, popup: "Abhishek Kumar"},
{lat: 24.94995291735982, lng: 118.6783729252374, popup: "Kaige Chen"},
{lat: 43.15951170805985, lng: -79.24680006638461, popup: "Skye Reid"},
{lat: 37.509975093912196, lng: 71.59398400486116, popup: "Sarfaroz Yunusov"},
];
locations.forEach(function(location) {
L.marker([location.lat, location.lng]).addTo(map)
.bindPopup(location.popup);
});
&lt;/script>
&lt;h1 id="join-us">Join Us&lt;/h1>
&lt;p>We are recruiting new graduate students for Fall, 2024&lt;/p>
&lt;p>&lt;strong>Undergraduates:&lt;/strong> Please don&amp;rsquo;t hesitate to email me to inquire about research projects that I (or better, yet, you) may have in mind. Please also attach your transcript as well as a brief description of which areas of my research interests (e.g., natural language processing) you would like to work on and why. I highly encourage, and prefer, students that are planning on a summer internship (under the NSERC USRA or SURA program), or are planning to do an Honour&amp;rsquo;s thesis.&lt;/p>
&lt;p>&lt;strong>Graduates:&lt;/strong> M.Sc. (&lt;!-- raw HTML omitted -->Computer Science&lt;!-- raw HTML omitted -->) and PhD (&lt;!-- raw HTML omitted -->Intelligent Systems and Data Science&lt;!-- raw HTML omitted -->) admissions are handled centrally in our department. Please see &lt;a href="https://brocku.ca/graduate-studies/future-students/apply/" target="_blank" rel="noopener">this&lt;/a> page for application instructions.&lt;/p></description></item></channel></rss>