@inproceedings{trichelair-etal-2019-reasonable,
    title = "How Reasonable are Common-Sense Reasoning Tasks: A Case-Study on the {W}inograd Schema Challenge and {SWAG}",
    author = "Trichelair, Paul  and
      Emami, Ali  and
      Trischler, Adam  and
      Suleman, Kaheer  and
      Cheung, Jackie Chi Kit",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1335",
    doi = "10.18653/v1/D19-1335",
    pages = "3382--3387",
    abstract = "Recent studies have significantly improved the state-of-the-art on common-sense reasoning (CSR) benchmarks like the Winograd Schema Challenge (WSC) and SWAG. The question we ask in this paper is whether improved performance on these benchmarks represents genuine progress towards common-sense-enabled systems. We make case studies of both benchmarks and design protocols that clarify and qualify the results of previous work by analyzing threats to the validity of previous experimental designs. Our protocols account for several properties prevalent in common-sense benchmarks including size limitations, structural regularities, and variable instance difficulty.",
}